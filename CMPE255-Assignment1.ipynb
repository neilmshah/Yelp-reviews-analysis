{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract restuarant business IDs from business.json \n",
    "def filter_restaurants():\n",
    "    print(\"Filtering Restaurants..\")\n",
    "    restaurants = open('restaurants.json', 'w')\n",
    "    with open('./business.json') as f:\n",
    "        for business in f:\n",
    "            business_data = json.loads(business)\n",
    "            business_id = business_data['business_id']\n",
    "            categories = business_data['categories']\n",
    "            if categories and 'Restaurants' in categories:\n",
    "                restaurants.write(json.dumps(\n",
    "                    {'business_id': business_id, 'stars': business_data['stars']}))\n",
    "                restaurants.write('\\n')\n",
    "    f.close()\n",
    "    restaurants.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sort the reviews by business ID \n",
    "def sortData(filename):\n",
    "    print(\"Sorting Reviews by Restaurant ID..\")\n",
    "    reviews = []\n",
    "    with open(filename) as f:\n",
    "        for i in f: reviews.append(json.loads(i))\n",
    "    f.close()\n",
    "    reviews.sort(key=operator.itemgetter('business_id'))\n",
    "    with open(filename, 'w') as f:\n",
    "        for r in reviews:\n",
    "            f.write(json.dumps(r))\n",
    "            f.write('\\n')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new dataset of 15000 reviews\n",
    "# as program crashes if there are too many\n",
    "def extract_json():\n",
    "    filter_restaurants()\n",
    "\n",
    "    print(\"Extracting reviews based on Restaurant ID..\")\n",
    "\n",
    "    traning_set = open('labeled_reviews.json', 'w')\n",
    "    restaurants = []\n",
    "    \n",
    "    with open('./restaurants.json') as rest:\n",
    "        for i in rest:\n",
    "            rest_data = json.loads(i)\n",
    "            restaurants.append(rest_data['business_id'])\n",
    "    \n",
    "    with open('./review.json') as f:\n",
    "        reviewCount=0\n",
    "        for review in f:\n",
    "            if(reviewCount==15000): break\n",
    "            data = json.loads(review)\n",
    "            if data['business_id'] not in restaurants: continue\n",
    "\n",
    "            currReview = {\n",
    "                'business_id': data['business_id'], 'text': (data['text']).replace('\\n', ' ').replace('\\r', '').strip()}\n",
    "            traning_set.write(json.dumps(currReview))\n",
    "            traning_set.write('\\n')\n",
    "            reviewCount += 1\n",
    "\n",
    "    traning_set.close()\n",
    "    sortData('./labeled_reviews.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering Restaurants..\n",
      "Extracting reviews based on Restaurant ID..\n",
      "Sorting Reviews by Restaurant ID..\n"
     ]
    }
   ],
   "source": [
    "extract_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labeling reviews as Positive/Negative/Neutral..\n"
     ]
    }
   ],
   "source": [
    "# Label each review as postive, negative or neutral based on \n",
    "# positive/negative wordset from postive.txt and negative.txt\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def labelReviews(filename):\n",
    "\n",
    "    print(\"Labeling reviews as Positive/Negative/Neutral..\")\n",
    "\n",
    "    positiveWords=[]\n",
    "    negativeWords=[]\n",
    "    data = []\n",
    "    corpus=[]\n",
    "    wordsData=[]\n",
    "\n",
    "    with open('./positiveWordset.txt') as f:\n",
    "        for line in f: positiveWords.append(line.replace(\"\\n\",\"\"))\n",
    "\n",
    "    with open('./negativeWordset.txt') as f:\n",
    "        for line in f: negativeWords.append(line.replace(\"\\n\",\"\"))\n",
    "\n",
    "    with open(filename) as f:\n",
    "        for line in f: data.append(json.loads(line))\n",
    "\n",
    "    for d in data: corpus.append(d[\"text\"])\n",
    "\n",
    "    for c in corpus:\n",
    "        vectorizer = CountVectorizer(stop_words=\"english\")\n",
    "        X = vectorizer.fit_transform([c])\n",
    "        wordsData.append(vectorizer.get_feature_names())\n",
    "\n",
    "\n",
    "    for index,d in enumerate(wordsData):\n",
    "        posAggregate=sum(el in d for el in positiveWords)\n",
    "        negAggregate=sum(el in d for el in negativeWords)\n",
    "        \n",
    "        if(posAggregate - negAggregate < -1): data[index][\"label\"]=\"Negative\"\n",
    "        elif(posAggregate - negAggregate > 1): data[index][\"label\"] = \"Positive\"\n",
    "        else: data[index][\"label\"] = \"Neutral\"\n",
    "\n",
    "    with open(filename,'w') as f:\n",
    "        for d in data:\n",
    "            f.write(json.dumps(d))\n",
    "            f.write(\"\\n\")\n",
    "    f.close()\n",
    "\n",
    "labelReviews('./labeled_reviews.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Analysis Program Begins\n",
    "\n",
    "def load_data():\n",
    "    data = []\n",
    "    data_labels = []\n",
    "    data_id = []\n",
    "\n",
    "    with open(\"./labeled_reviews.json\") as f:\n",
    "        for i in f:\n",
    "            review = json.loads(i)\n",
    "            data.append(review['text'])\n",
    "            data_labels.append(review['label'])\n",
    "            data_id.append(review['business_id'])\n",
    "\n",
    "    return data, data_labels, data_id\n",
    "\n",
    "\n",
    "data, data_labels, data_id = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_to_features(data):\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    vectorizer = CountVectorizer(\n",
    "        analyzer='word',\n",
    "        lowercase=False,\n",
    "    )\n",
    "    features = vectorizer.fit_transform(\n",
    "        data\n",
    "    )\n",
    "    features_nd = features.toarray()\n",
    "    return features_nd\n",
    "\n",
    "features_nd = transform_to_features(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/model_selection/_split.py:2069: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:459: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table><tr><td>Business ID</td><td>Prediction</td><td>Actual Rating</td></tr><tr><td>Xg5qEQiB-7L6kGJ5F4K3bQ</td><td>positive</td><td>5.0</td></tr><tr><td>yGMCl0vYigshkXiZFIDTNw</td><td>negative</td><td>3.0</td></tr><tr><td>oFHvr1cAktvU-bQgrl4aPw</td><td>positive</td><td>4.0</td></tr><tr><td>k2b3niokS_tosjah_rzCPw</td><td>positive</td><td>3.5</td></tr><tr><td>r48H_sNUGmcRGX1LsEc2mg</td><td>positive</td><td>3.0</td></tr><tr><td>75RP4HSsSJOe_e7e2e3jQQ</td><td>neutral</td><td>4.0</td></tr><tr><td>f_eiOrEcMnkHB7GvQVOHkQ</td><td>positive</td><td>4.0</td></tr><tr><td>YRyYbOSwvHkZsZOLv98oQg</td><td>positive</td><td>4.0</td></tr><tr><td>Cdk3wRR7TwJb1JW7agPJXw</td><td>neutral</td><td>4.0</td></tr><tr><td>CJKgffwOSqZ7WpKVruA0xQ</td><td>positive</td><td>3.5</td></tr><tr><td>1hqOjPxgH8IXE4bNq6DFiw</td><td>positive</td><td>3.5</td></tr><tr><td>kNTOkEtBVW14RpFEUREzEw</td><td>positive</td><td>3.0</td></tr><tr><td>zuwba6QEBIDZT0tJZmNhdQ</td><td>positive</td><td>3.5</td></tr><tr><td>CuT2Xh3vAYvRfxUWlUo6oA</td><td>positive</td><td>4.0</td></tr><tr><td>WKOUTdVJS58E178JjhwidQ</td><td>positive</td><td>4.0</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.8176666666666667\n",
      "Precision = 0.7067339474662129\n",
      "Recall = 0.623658457978303\n",
      "F1-score = 0.6626024052361132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:459: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:459: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:459: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Score = [0.8067983 0.80225   0.8032008]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TRAINING AND PREDICTION MODEL\n",
    "\n",
    "def train_then_build_model(data_labels, features_nd, data, data_id):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        features_nd,\n",
    "        data_labels,\n",
    "        train_size=0.80,\n",
    "        random_state=1234)\n",
    "\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    log_model = LogisticRegression()\n",
    "\n",
    "    log_model = log_model.fit(X=X_train, y=y_train)\n",
    "    y_pred = log_model.predict(X_test)\n",
    "\n",
    "    restReview = {}\n",
    "\n",
    "    featuresList = features_nd.tolist()\n",
    "\n",
    "    for i in range(len(X_test)):\n",
    "        index = featuresList.index(X_test[i].tolist())\n",
    "        if(data_id[index] not in restReview.keys()):\n",
    "            restReview[data_id[index]] = {'positive': 0, 'neutral': 0, 'negative': 0}\n",
    "\n",
    "        if(y_pred[i] == 'Positive'): restReview[data_id[index]]['positive'] += 1\n",
    "        elif(y_pred[i] == 'Negative'): restReview[data_id[index]]['negative'] += 1\n",
    "        else: restReview[data_id[index]]['neutral'] += 1\n",
    "\n",
    "\n",
    "    overallReview={}\n",
    "    for id in restReview:\n",
    "        restReview[id]=sorted(restReview[id].items(), key=lambda x:x[1], reverse=True)\n",
    "        overallReview[id]=restReview[id][0][0]\n",
    "\n",
    "    yelpReview = {}\n",
    "    with open('./restaurants.json') as rest:\n",
    "        for i in rest:\n",
    "            yelp_r = json.loads(i)\n",
    "            yelpReview[yelp_r['business_id']] = yelp_r['stars']\n",
    "    \n",
    "    idCount, data = 0, [['Business ID',\"Prediction\", \"Actual Rating\"]]\n",
    "    for id in restReview:\n",
    "        if(idCount==15): break\n",
    "        data.append([id, overallReview[id], yelpReview[id]])\n",
    "        idCount+=1\n",
    "    \n",
    "    from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    import numpy as np\n",
    "    \n",
    "    from IPython.display import HTML, display\n",
    "\n",
    "    display(HTML(\n",
    "       '<table><tr>{}</tr></table>'.format(\n",
    "           '</tr><tr>'.join(\n",
    "               '<td>{}</td>'.format('</td><td>'.join(str(_) for _ in row)) for row in data)\n",
    "           )\n",
    "    ))\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    recall = np.diag(cm) / np.sum(cm, axis=1)\n",
    "    precision = np.diag(cm) / np.sum(cm, axis=0)\n",
    "    recall = np.mean(recall)\n",
    "    precision = np.mean(precision)\n",
    "    f1score = (2*precision*recall)/(precision+recall)\n",
    "    print(\"Accuracy = {}\".format(accuracy))\n",
    "    print(\"Precision = {}\".format(precision))\n",
    "    print(\"Recall = {}\".format(recall))\n",
    "    print(\"F1-score = {}\".format(f1score))\n",
    "    print(\"Cross Validation Score = {}\\n\".format(cross_val_score(\n",
    "        log_model, X_train, y_train, cv=3, scoring=\"accuracy\")))\n",
    "\n",
    "    \n",
    "train_then_build_model(data_labels, features_nd, data, data_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
